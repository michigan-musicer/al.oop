{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "1W8tz5NMk_rz"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVgYlqNh9Dwf"
      },
      "source": [
        "# ***TO USE***\n",
        "\n",
        "Go to File->Save a copy in Drive. This is the master copy, which non-admin members are not allowed to edit.\n",
        "\n",
        "***Before starting, use the shortcut Ctrl+] to collapse all cells in the notebook.*** This ensures that solutions will not be spoiled before you answer the coding questions in this notebook. You can safely unhide the outermost cells; solutions will still be hidden."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1W8tz5NMk_rz"
      },
      "source": [
        "# **Building a logistic regression model**\n",
        "\n",
        "This notebook will walk you through building a logistic regression model in Python."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMHZXQ2Vjr7F"
      },
      "source": [
        "# Importing\n",
        "\n",
        "`numpy` is a library (package of functions and methods) for scientific computing. For the most part, we'll be using it to work with matrices in Python.\n",
        "\n",
        "The `as np` part of the statement just changes the name we use to refer to `numpy`. Machine learning practitioners prefer the shorthand because it's less to type.\n",
        "\n",
        "\\\n",
        "\n",
        "Unhide the following cell, and run the code by either:\n",
        "- clicking the \"run\" button on the left of the cell\n",
        "- using the shortcut **Ctrl+Enter**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzGkCtM9LOI2"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32byofrCIMr8"
      },
      "source": [
        "# Before we start...\n",
        "\n",
        "What are the dimensions of $X$ and $y$ again?\n",
        "\n",
        "(Use the presentation's convention that $m$ is the number of instances and $n$ is the number of features.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYoE2uzCmZdu"
      },
      "source": [
        "### Solution:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVXX64Ermb_x"
      },
      "source": [
        "$X$ is $(n, m)$. $y$ is $(1, m)$.\n",
        "\n",
        "$$X = \\begin{bmatrix}\n",
        "| & | & | & ... & | \\\\\n",
        "| & | & | & ... & | \\\\\n",
        "x^{(1)} & x^{(2)} & x^{(3)} & ... & x^{(m)} \\\\\n",
        "| & | & | & ... & | \\\\\n",
        "| & | & | & ... & |\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "\\\\\n",
        "\n",
        "$$y = \\begin{bmatrix}\n",
        "y^{(1)} &  y^{(2)} &  y^{(3)} &  ... & y^{(m)}\n",
        "\\end{bmatrix}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bg98fX2RIXyz"
      },
      "source": [
        "# Initializing $w$ and $b$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hp7u3jGzCt4k"
      },
      "source": [
        "We will initialize $w$ to random values in the range $(0,1)$. (This means that all the random values are between 0 and 1, not including 0 or 1.) We will initialize $b$ to 0.\n",
        "\n",
        "(This is technically not necessary for logistic regression, but when we cover neural networks in a later lesson, this idea becomes important. Check out this video by Dr. Andrew Ng for more: https://www.youtube.com/watch?v=6by6Xas_Kho)\n",
        "\n",
        "\\\n",
        "\n",
        "#### **Introduction to library function:** [`np.random.rand(x0, x2)`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.rand.html)\n",
        "\n",
        "*Links above go to official Numpy documentation.*\n",
        "\n",
        "We will use the `rand()` function from `np.random` to generate random values. The following statement creates a matrix with dimensions $(\\textrm{d0},\\textrm{d1})$ where each element is a random value:\n",
        "\n",
        "```np.random.rand(d0, d1)```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubgi9PVZDDgU"
      },
      "source": [
        "# Use np.random.rand(..., ...) to initialize w.\n",
        "\n",
        "def initialize_weights_and_bias(n):\n",
        "\n",
        "  # START CODING HERE\n",
        "  w = None\n",
        "  b = None\n",
        "  # END CODING HERE\n",
        "\n",
        "  return w, b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fB_jG_RBu98"
      },
      "source": [
        "### Solution:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KC8A9k_oB-Vh"
      },
      "source": [
        "def initialize_weights_and_bias_sol(n):\n",
        "\n",
        "  w = np.random.rand(n, 1)\n",
        "  b = 0\n",
        "\n",
        "  return w, b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zU0jEk-9KZe4"
      },
      "source": [
        "# The sigmoid function\n",
        "\n",
        "Recall that the sigmoid or logistic function is defined as\n",
        "\n",
        "$$a = \\frac{1}{1 + e^{-z}}$$\n",
        "\n",
        "#### **Introduction to library function:** [`np.exp(x)`](https://numpy.org/doc/stable/reference/generated/numpy.exp.html)\n",
        "\n",
        "*Link above goes to official Numpy documentation.*\n",
        "\n",
        "`np.exp(x)` simply returns $e^x$. You will get an error if you try to write `e ** x`, so use the library funciton.\n",
        "\n",
        "\\\n",
        "\n",
        "We will use `sigmoid` to calculate $a$ for every example. You can simply pass in the vector $z$ as if it is a scalar value; Numpy uses *broadcasting* to simplify writing code with vectors and matrices.\n",
        "\n",
        "Essentially, *broadcasting* applies operations on all elements in a vector or matrix. For example, if you have a matrix `x` set to\n",
        "\n",
        "$$\\begin{bmatrix} 1 & 1 \\\\ 2 & 5 \\end{bmatrix}$$\n",
        "\n",
        "and you set `y = x + 1`, `y` will be set to\n",
        "\n",
        "$$\\begin{bmatrix} 2 & 2 \\\\ 3 & 6 \\end{bmatrix}$$\n",
        "\n",
        "Importantly, we can also do this with compatible vectors and matrices. So if we have `a` set to\n",
        "\n",
        "$$\\begin{bmatrix} 1 & 9 & 8 \\\\ 4 & 0 & 0 \\end{bmatrix}$$\n",
        "\n",
        "and `b` set to\n",
        "\n",
        "$$\\begin{bmatrix} 3 & 5 & 1 \\end{bmatrix}$$\n",
        "\n",
        "then set `c = a + b`, `c` will be set to\n",
        "\n",
        "$$\\begin{bmatrix} 4 & 14 & 9 \\\\ 7 & 5 & 1 \\end{bmatrix}$$\n",
        "\n",
        "If you want more clarification, you can check out either [this Andrew Ng video](https://www.youtube.com/watch?v=tKcLaGdvabM) or the [official Numpy documentation](https://numpy.org/devdocs/user/theory.broadcasting.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkWeeGC2LSLi"
      },
      "source": [
        "# Numpy broadcasting means you can write the sigmoid function in just one line\n",
        "\n",
        "def sigmoid(z):\n",
        "\n",
        "  # START CODING HERE\n",
        "  a = None\n",
        "  # END CODING HERE\n",
        "\n",
        "  return a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfAL4VqnLpWC"
      },
      "source": [
        "### Solution:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYO25NfCLq_F"
      },
      "source": [
        "def sigmoid_sol(z):\n",
        "\n",
        "  a = 1 / (1 + np.exp(-1 * z))\n",
        "\n",
        "  return a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prORc6OJKKzA"
      },
      "source": [
        "# Forward propagation: calculating $Z$ and $A$\n",
        "\n",
        "Recall that $z$ is calculated as\n",
        "\n",
        "$$w^TX + b$$\n",
        "\n",
        "To get the transpose of a matrix `a`, simply use `a.T`.\n",
        "\n",
        "In Numpy, we use the `@` operator to perform matrix multiplication. (In ancient times, we used a function called [`np.dot(a, b)`](https://numpy.org/doc/stable/reference/generated/numpy.dot.html), but the `@` operator was introduced to make code more readable.)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWSt8InzL1LF"
      },
      "source": [
        "# use the transpose method and the @ operator to calculate z\n",
        "# Use a previously defined function to calculate a\n",
        "\n",
        "def forward_prop(X, w, b):\n",
        "\n",
        "  # START CODING HERE\n",
        "  z = None\n",
        "  a = None\n",
        "  # END CODING HERE\n",
        "\n",
        "  return z, a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6j5nvp97L1lk"
      },
      "source": [
        "### Solution:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqVGBXFqMJPB"
      },
      "source": [
        "def forward_prop_sol(X, w, b):\n",
        "\n",
        "  z = w.T @ X + b\n",
        "  a = sigmoid(z)\n",
        "\n",
        "  return z, a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyErM2HIKcRA"
      },
      "source": [
        "# Calculating the cost\n",
        "\n",
        "Recall that the cost function $J$ is\n",
        "\n",
        "$$\\frac{1}{m} \\sum_{i=0}^{m} -[y\\log(a) + (1-y)\\log(1-a)]$$\n",
        "\n",
        "Numpy broadcasting allows us to quickly write code to compute the costs on individual instances. Try to figure out how on your own! (Hint: you don't need to separately take the logarithm for all $m$ outputs $y$.)\n",
        "\n",
        "Once we compute the costs on individual instances, we can use a library function to sum the elements in the resulting vector:\n",
        "\n",
        "#### **Introduction to library function:** [`numpy.sum(a)`](https://numpy.org/doc/stable/reference/generated/numpy.sum.html)\n",
        "\n",
        "Return the sum of the elements a vector (or a matrix along an axis -- discussed in neural network lesson). If `a` is set to\n",
        "\n",
        "$$\\begin{bmatrix} 1 & 2 & 3 & 4 \\end{bmatrix}$$\n",
        "\n",
        "then `np.sum(a)` simply returns $10$.\n",
        "\n",
        "\\\n",
        "\n",
        "We can divide the sum from `np.sum(...)` by $m$ to get our final cost."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ghcxd032Me4L"
      },
      "source": [
        "# Use broadcasting and np.sum(...) to calculate cost\n",
        "# Note that the value of m is already calculated from the dimensions of Y\n",
        "\n",
        "def calculate_cost(y, a):\n",
        "\n",
        "  m = y.shape[1]\n",
        "\n",
        "  # START CODING HERE\n",
        "  cost = None\n",
        "  # END CODING HERE\n",
        "\n",
        "  return cost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEeb4ncLMfCk"
      },
      "source": [
        "### Solution:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYrUL6C7MfId"
      },
      "source": [
        "def calculate_cost_sol(y, a):\n",
        "\n",
        "  m = y.shape[1]\n",
        "\n",
        "  cost = np.sum(-(y * np.log(a) + (1-y) * np.log(1-a)) ) / m\n",
        "\n",
        "  return cost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1xET-hEKjC9"
      },
      "source": [
        "# Back propagation: calculating derivatives and updating weights and bias\n",
        "\n",
        "Recall that since all the derivatives we take are with respect to $J$, we just use the denominators as our variable names (i.e. for today `dz`, `dw`, `db`).\n",
        "\n",
        "Recall the following definitions:\n",
        "\n",
        "$$dz = y - a$$\n",
        "\n",
        "$$dw = \\frac{1}{m} (Xdz^T)$$\n",
        "\n",
        "$$db = \\frac{1}{m} \\sum_{i=0}^{m} dz^{(i)}$$\n",
        "\n",
        "Finish the back propagation function by updating $w$ and $b$. Don't forget to multiply by the learning rate $\\alpha$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jl0S0QjyOAg8"
      },
      "source": [
        "def back_prop(y, a, z, w, b, learning_rate=0.0001):\n",
        "\n",
        "  m = y.shape[1]\n",
        "\n",
        "  # START CODING HERE\n",
        "  # calculating derivatives\n",
        "  dz = None\n",
        "  dw = None\n",
        "  db = None\n",
        "\n",
        "  # updating w and b\n",
        "  w = None\n",
        "  b = None\n",
        "  # END CODING HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ungb27KyOAv2"
      },
      "source": [
        "### Solution:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaVUgNQwOA06"
      },
      "source": [
        "def back_prop_sol(y, a, z, w, b, learning_rate=0.0001):\n",
        "\n",
        "  m = y.shape[1]\n",
        "\n",
        "  # calculating derivatives\n",
        "  dz = y - a\n",
        "  dw = X @ dz.T / m\n",
        "  db = np.sum(dz) / m\n",
        "\n",
        "  # updating w and b\n",
        "  w = w - learning_rate * dw\n",
        "  b = b - learning_rate * db"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbmebjtNKxOC"
      },
      "source": [
        "# Implementing the entire model\n",
        "\n",
        "Use the functions you have wrote previously to:\n",
        "- initialize $w$ and $b$,\n",
        "- run forward propagation to get values of $z$ and $a$,\n",
        "- calculate the cost $J$ based on current values of $w$ and $b$, and\n",
        "- run back propagation to update $w$ and $b$ using gradient descent.\n",
        "\n",
        "This will run in a loop several times so that $w$ and $b$ are set to the correct values to fit the input data $X$.\n",
        "\n",
        "Congratulations! You have successfully implemented logistic regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqWLA02vQI7p"
      },
      "source": [
        "def run(X, y, learning_rate=0.0001, num_iters=10000):\n",
        "\n",
        "  m = X.shape[1]\n",
        "  n = X.shape[0]\n",
        "\n",
        "  for i in range(num_iters):\n",
        "    # START CODE HERE\n",
        "    w, b = None\n",
        "    z, a = None\n",
        "    cost = None\n",
        "    None # what function do you still need to run?\n",
        "    # END CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrzhvnzgQJG3"
      },
      "source": [
        "### Solution:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdMQlwWmQJMy"
      },
      "source": [
        "def run_sol(X, Y, learning_rate=0.0001, num_iters=10000):\n",
        "\n",
        "  m = X.shape[1]\n",
        "  n = X.shape[0]\n",
        "\n",
        "  for i in range(num_iters):\n",
        "    w, b = initialize_weights_and_bias(n)\n",
        "    z, a = forward_prop(X, w, b)\n",
        "    cost = calculate_cost(y, a)\n",
        "    back_prop(y, a, z, w, b, learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}